{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_dict = {}\n",
    "for word in words:\n",
    "    chs = ['<S>'] + list(word) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        bigram_dict[bigram] = bigram_dict.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('n', '<E>'), 6763),\n",
       " (('a', '<E>'), 6640),\n",
       " (('a', 'n'), 5438),\n",
       " (('<S>', 'a'), 4410),\n",
       " (('e', '<E>'), 3983),\n",
       " (('a', 'r'), 3264),\n",
       " (('e', 'l'), 3248),\n",
       " (('r', 'i'), 3033),\n",
       " (('n', 'a'), 2977),\n",
       " (('<S>', 'k'), 2963)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(bigram_dict.items(), key = lambda kv: -kv[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# Pre-process the data, creating a map of strings to ints to convert the input\n",
    "# as well as an int to string mapping to convert the output\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = { s: i+1 for i,s in enumerate(chars) }\n",
    "stoi['.'] = 0\n",
    "itos = { i: s for s,i in stoi.items() }\n",
    "\n",
    "print(stoi)\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". e\n",
      "e m\n",
      "m m\n",
      "m a\n",
      "a .\n"
     ]
    }
   ],
   "source": [
    "# Bigrams kinda suck\n",
    "# Shall we use a neural network?\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        rowIdx = stoi[ch1]\n",
    "        colIdx = stoi[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(rowIdx)\n",
    "        ys.append(colIdx)\n",
    "        \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Xs are the index of the first character in the bigram at each index\n",
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ys are the index of the second character in the bigram at each index\n",
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To feed it into the NN, we need to encode it using one hot encoding as floats\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yenc = F.one_hot(ys, num_classes=27).float()\n",
    "yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7b784d18d0>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f7b7852a7a0>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(yenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674],\n",
       "        [-0.2373],\n",
       "        [-0.0274],\n",
       "        [-1.1008],\n",
       "        [ 0.2859],\n",
       "        [-0.0296]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As an example, create weights for one neuron:\n",
    "# Define weights for first neuron \n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,1), generator=g) # 27 features / 1 neuron\n",
    "W[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674, -0.2373, -0.0274, -1.1008,  0.2859, -0.0296, -1.5471,  0.6049,\n",
       "          0.0791,  0.9046, -0.4713,  0.7868, -0.3284, -0.4330,  1.3729,  2.9334,\n",
       "          1.5618, -1.6261,  0.6772, -0.8404,  0.9849, -0.1484, -1.4795,  0.4483,\n",
       "         -0.0707,  2.4968,  2.4448],\n",
       "        [-0.6701, -1.2199,  0.3031, -1.0725,  0.7276,  0.0511,  1.3095, -0.8022,\n",
       "         -0.8504, -1.8068,  1.2523, -1.2256,  1.2165, -0.9648, -0.2321, -0.3476,\n",
       "          0.3324, -1.3263,  1.1224,  0.5964,  0.4585,  0.0540, -1.7400,  0.1156,\n",
       "          0.8032,  0.5411, -1.1646],\n",
       "        [ 0.1476, -1.0006,  0.3801,  0.4733, -0.9103, -0.7830,  0.1351, -0.2116,\n",
       "         -1.0406, -1.5367,  0.9374, -0.8830,  1.7457,  2.1346, -0.8561,  0.5408,\n",
       "          0.6169,  1.5160, -1.0447, -0.6641, -0.7239,  1.7507,  0.1753,  0.9928,\n",
       "         -0.6279,  0.0770, -1.1641]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define weights for first neuron\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27,27), generator=g) # 27 features / 27 neurons\n",
    "W[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5674e+00, -2.3729e-01, -2.7385e-02, -1.1008e+00,  2.8588e-01,\n",
       "         -2.9644e-02, -1.5471e+00,  6.0489e-01,  7.9136e-02,  9.0462e-01,\n",
       "         -4.7125e-01,  7.8682e-01, -3.2844e-01, -4.3297e-01,  1.3729e+00,\n",
       "          2.9334e+00,  1.5618e+00, -1.6261e+00,  6.7716e-01, -8.4040e-01,\n",
       "          9.8488e-01, -1.4837e-01, -1.4795e+00,  4.4830e-01, -7.0731e-02,\n",
       "          2.4968e+00,  2.4448e+00],\n",
       "        [ 4.7236e-01,  1.4830e+00,  3.1748e-01,  1.0588e+00,  2.3982e+00,\n",
       "          4.6827e-01, -6.5650e-01,  6.1662e-01, -6.2198e-01,  5.1007e-01,\n",
       "          1.3563e+00,  2.3445e-01, -4.5585e-01, -1.3132e-03, -5.1161e-01,\n",
       "          5.5570e-01,  4.7458e-01, -1.3867e+00,  1.6229e+00,  1.7197e-01,\n",
       "          9.8846e-01,  5.0657e-01,  1.0198e+00, -1.9062e+00, -4.2753e-01,\n",
       "         -2.1259e+00,  9.6041e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [ 1.9359e-01,  1.0532e+00,  6.3393e-01,  2.5786e-01,  9.6408e-01,\n",
       "         -2.4855e-01,  2.4756e-02, -3.0404e-02,  1.5622e+00, -4.4852e-01,\n",
       "         -1.2345e+00,  1.1220e+00, -6.7381e-01,  3.7882e-02, -5.5881e-01,\n",
       "         -8.2709e-01,  8.2253e-01, -7.5100e-01,  9.2778e-01, -1.4849e+00,\n",
       "         -2.1293e-01, -1.1860e+00, -6.6092e-01, -2.3348e-01,  1.5447e+00,\n",
       "          6.0061e-01, -7.0909e-01],\n",
       "        [-6.7006e-01, -1.2199e+00,  3.0314e-01, -1.0725e+00,  7.2762e-01,\n",
       "          5.1114e-02,  1.3095e+00, -8.0220e-01, -8.5042e-01, -1.8068e+00,\n",
       "          1.2523e+00, -1.2256e+00,  1.2165e+00, -9.6478e-01, -2.3211e-01,\n",
       "         -3.4762e-01,  3.3244e-01, -1.3263e+00,  1.1224e+00,  5.9641e-01,\n",
       "          4.5846e-01,  5.4011e-02, -1.7400e+00,  1.1560e-01,  8.0319e-01,\n",
       "          5.4108e-01, -1.1646e+00]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiply xenc by Weights\n",
    "# (5<-, 27) @ (27, 27<-) = (5, 27)\n",
    "# Dot product = multiply then add\n",
    "xenc @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "tensor([-0.4713,  1.2523,  0.9374,  0.1467,  0.9019,  1.3563,  0.9737, -0.4958,\n",
      "        -1.2542, -0.3907, -0.0492, -1.5264, -1.3270, -1.2345, -0.7517, -2.1007,\n",
      "         0.1124, -0.2929, -1.0672, -0.9939,  0.6408, -0.1471, -1.0143, -0.3921,\n",
      "         1.4224, -0.5996, -0.6348])\n",
      "tensor(-1.2345)\n",
      "tensor(-1.2345)\n"
     ]
    }
   ],
   "source": [
    "# In order to demonstrate the dot product\n",
    "# we look at one input feature and its corresponding weights at each neuron:\n",
    "print(xenc[3])\n",
    "print(W[:, 10])\n",
    "\n",
    "# Same as (xenc @ W)[3, 10]\n",
    "dot_product = (xenc[3] * W[:, 10]).sum()\n",
    "print(dot_product)\n",
    "print((xenc @ W)[3, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.7940,  0.7888,  0.9730,  0.3326,  1.3309,  0.9708,  0.2129,  1.8311,\n",
       "          1.0824,  2.4710,  0.6242,  2.1964,  0.7200,  0.6486,  3.9469, 18.7908,\n",
       "          4.7673,  0.1967,  1.9683,  0.4315,  2.6775,  0.8621,  0.2277,  1.5656,\n",
       "          0.9317, 12.1434, 11.5281],\n",
       "        [ 1.6038,  4.4060,  1.3737,  2.8830, 11.0032,  1.5972,  0.5187,  1.8527,\n",
       "          0.5369,  1.6654,  3.8818,  1.2642,  0.6339,  0.9987,  0.5995,  1.7432,\n",
       "          1.6073,  0.2499,  5.0680,  1.1876,  2.6871,  1.6596,  2.7728,  0.1486,\n",
       "          0.6521,  0.1193,  2.6128],\n",
       "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
       "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
       "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
       "          4.6866,  1.8232,  0.4921],\n",
       "        [ 1.2136,  2.8669,  1.8850,  1.2942,  2.6224,  0.7799,  1.0251,  0.9701,\n",
       "          4.7691,  0.6386,  0.2910,  3.0710,  0.5098,  1.0386,  0.5719,  0.4373,\n",
       "          2.2763,  0.4719,  2.5289,  0.2265,  0.8082,  0.3054,  0.5164,  0.7918,\n",
       "          4.6866,  1.8232,  0.4921],\n",
       "        [ 0.5117,  0.2953,  1.3541,  0.3422,  2.0701,  1.0524,  3.7043,  0.4483,\n",
       "          0.4272,  0.1642,  3.4984,  0.2936,  3.3753,  0.3811,  0.7929,  0.7064,\n",
       "          1.3944,  0.2655,  3.0723,  1.8156,  1.5816,  1.0555,  0.1755,  1.1225,\n",
       "          2.2327,  1.7179,  0.3120]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neurons are giving us logits (log counts), which we have to exponentiate https://www.wolframalpha.com/input?i=exp%28x%29\n",
    "# This normalizes the data into counts, like the bigram model, getting rid of the negative numbers\n",
    "# \n",
    "logits = xenc @ W\n",
    "counts = logits.exp() # equivalent to N in bigram model\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
       "         0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
       "         0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459],\n",
       "        [0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
       "         0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
       "         0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
       "         0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
       "         0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126],\n",
       "        [0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
       "         0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
       "         0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the probability distribution by summing and dividing by number of weights:\n",
    "# Also known as softmax\n",
    "# NOTE: all of these computations are differentiable, i.e. we can back propagate through them.\n",
    "# We interpret this as the probability of the next character given the input character\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "print(probs.shape)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "bigram example 1\n",
      "x (input): . | y (label): e (indexes 0,5)\n",
      "output probabilities from the neural net:  tensor([0.0607, 0.0100, 0.0123, 0.0042, 0.0168, 0.0123, 0.0027, 0.0232, 0.0137,\n",
      "        0.0313, 0.0079, 0.0278, 0.0091, 0.0082, 0.0500, 0.2378, 0.0603, 0.0025,\n",
      "        0.0249, 0.0055, 0.0339, 0.0109, 0.0029, 0.0198, 0.0118, 0.1537, 0.1459])\n",
      "label (actual next character):  5\n",
      "probability assigned by the network to the correct character:  0.012286253273487091\n",
      "log likelihood: -4.3992743492126465\n",
      "negative log likelihood: 4.3992743492126465\n",
      "\n",
      "----------\n",
      "bigram example 2\n",
      "x (input): e | y (label): m (indexes 5,13)\n",
      "output probabilities from the neural net:  tensor([0.0290, 0.0796, 0.0248, 0.0521, 0.1989, 0.0289, 0.0094, 0.0335, 0.0097,\n",
      "        0.0301, 0.0702, 0.0228, 0.0115, 0.0181, 0.0108, 0.0315, 0.0291, 0.0045,\n",
      "        0.0916, 0.0215, 0.0486, 0.0300, 0.0501, 0.0027, 0.0118, 0.0022, 0.0472])\n",
      "label (actual next character):  13\n",
      "probability assigned by the network to the correct character:  0.018050702288746834\n",
      "log likelihood: -4.014570713043213\n",
      "negative log likelihood: 4.014570713043213\n",
      "\n",
      "----------\n",
      "bigram example 3\n",
      "x (input): m | y (label): m (indexes 13,13)\n",
      "output probabilities from the neural net:  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character):  13\n",
      "probability assigned by the network to the correct character:  0.026691533625125885\n",
      "log likelihood: -3.623408794403076\n",
      "negative log likelihood: 3.623408794403076\n",
      "\n",
      "----------\n",
      "bigram example 4\n",
      "x (input): m | y (label): a (indexes 13,1)\n",
      "output probabilities from the neural net:  tensor([0.0312, 0.0737, 0.0484, 0.0333, 0.0674, 0.0200, 0.0263, 0.0249, 0.1226,\n",
      "        0.0164, 0.0075, 0.0789, 0.0131, 0.0267, 0.0147, 0.0112, 0.0585, 0.0121,\n",
      "        0.0650, 0.0058, 0.0208, 0.0078, 0.0133, 0.0203, 0.1204, 0.0469, 0.0126])\n",
      "label (actual next character):  1\n",
      "probability assigned by the network to the correct character:  0.07367684692144394\n",
      "log likelihood: -2.6080667972564697\n",
      "negative log likelihood: 2.6080667972564697\n",
      "\n",
      "----------\n",
      "bigram example 5\n",
      "x (input): a | y (label): . (indexes 1,0)\n",
      "output probabilities from the neural net:  tensor([0.0150, 0.0086, 0.0396, 0.0100, 0.0606, 0.0308, 0.1084, 0.0131, 0.0125,\n",
      "        0.0048, 0.1024, 0.0086, 0.0988, 0.0112, 0.0232, 0.0207, 0.0408, 0.0078,\n",
      "        0.0899, 0.0531, 0.0463, 0.0309, 0.0051, 0.0329, 0.0654, 0.0503, 0.0091])\n",
      "label (actual next character):  0\n",
      "probability assigned by the network to the correct character:  0.0149775305762887\n",
      "log likelihood: -4.201204299926758\n",
      "negative log likelihood: 4.201204299926758\n",
      "\n",
      "==========\n",
      "average negative log likelihood, i.e. loss = 3.7693049907684326\n"
     ]
    }
   ],
   "source": [
    "negative_lls = torch.zeros(5)\n",
    "\n",
    "for i in range(5):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print('-' * 10)\n",
    "    print(f'bigram example {i+1}')\n",
    "    print(f'x (input): {itos[x]} | y (label): {itos[y]} (indexes {x},{y})')\n",
    "    print('output probabilities from the neural net: ', probs[i])\n",
    "    print('label (actual next character): ', y)\n",
    "    prob = probs[i, y]\n",
    "    print('probability assigned by the network to the correct character: ', prob.item())\n",
    "    log_prob = torch.log(prob)\n",
    "    print('log likelihood:', log_prob.item())\n",
    "    negative_ll = -log_prob\n",
    "    print('negative log likelihood:', negative_ll.item())\n",
    "    negative_lls[i] = negative_ll\n",
    "    print()\n",
    "    \n",
    "print('=' * 10)\n",
    "print('average negative log likelihood, i.e. loss =', negative_lls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
